# data is pulled from the database storage and transformed
import boto3
import pandas as pd
import json
import io

### Extraction from S3 ### 
# Extract data from S3 bucket and return as a DataFrame
def extract_data_from_s3(bucket_name, file_name):
    # Initialize S3 client
    s3 = boto3.client("s3")

    # Download file from S3 bucket
    obj = s3.get_object(Bucket=bucket_name, Key=file_name)
    data = obj['Body'].read().decode('utf-8')

    # Parse JSON string into Python object
    data_json = json.loads(data)

    # Read data into DataFrame
    df = pd.DataFrame(data_json)
    return df

# AWS S3 settings
bucket_name = "cis9440-hw-raw-data"
file_name = "pedestrian_count_data.json"

# Extract data from S3 bucket
data_df = extract_data_from_s3(bucket_name, file_name)

#print(data_df.head())



### Transformation ###
def transform_data(data_df):

    # Convert columns to appropriate data types
    data_df = convert_data_types(data_df)

    # Extract date components and create a unified date format column
    data_df = extract_and_format_dates(data_df)

    # Drop unnecessary columns
    data_df = drop_columns(data_df)

    # Rename 'weather_summary' column to 'weather'
    data_df.rename(columns={'weather_summary': 'weather'}, inplace=True)

    # Drop duplicates
    data_df = drop_duplicates(data_df)

    return data_df


def convert_data_types(data_df):
    data_df['pedestrians'] = pd.to_numeric(data_df['pedestrians'], errors='coerce')
    data_df['towards_manhattan'] = pd.to_numeric(data_df['towards_manhattan'], errors='coerce')
    data_df['towards_brooklyn'] = pd.to_numeric(data_df['towards_brooklyn'], errors='coerce')
    return data_df


def extract_and_format_dates(data_df):
    data_df['hour_beginning'] = pd.to_datetime(data_df['hour_beginning'])
    data_df['year'] = data_df['hour_beginning'].dt.year
    data_df['quarter'] = data_df['hour_beginning'].dt.quarter
    data_df['month'] = data_df['hour_beginning'].dt.month
    data_df['day'] = data_df['hour_beginning'].dt.day
    data_df['hour'] = data_df['hour_beginning'].dt.hour
    data_df['date'] = data_df['hour_beginning'].dt.strftime('%Y-%m-%d')
    return data_df

# Dropped unnecessary columns that dont offer much insight
def drop_columns(data_df):
    data_df.drop(columns=['hour_beginning', 'location'], inplace=True)
    return data_df


def drop_duplicates(data_df):
    data_df_str = data_df.astype(str)
    data_df_str.drop_duplicates(inplace=True)
    data_df = data_df_str.astype(data_df.dtypes.to_dict())
    return data_df

# Transform the data and columns
transformed_data_df = transform_data(data_df)

print(len(transformed_data_df))

def handle_missing_values(data_df):
    data_df.dropna(subset=['pedestrians', 'towards_manhattan', 'towards_brooklyn', 'weather', 'temperature', 'precipitation'], inplace=True) 
    return data_df

# Handle values that do not show up as NaN
transformed_data_df.replace('nan', pd.NA, inplace=True)

transformed_data_df = handle_missing_values(transformed_data_df)
#print(transformed_data_df.head())
print(len(transformed_data_df))

# Reorganize columns
def reorganize(data_df):
    # Define the columns related to date types
    new_order = ['date','year', 'quarter', 'month', 'day', 'hour', 'pedestrians', 'towards_manhattan', 'towards_brooklyn', 'weather', 'temperature', 'precipitation', 'events']
    data_df = data_df[new_order]

    return data_df

transformed_data_df = reorganize(transformed_data_df)

#print(transformed_data_df.head())

### Mapping by generating IDs ###
def generate_ids(data_df):
    data_df['time_id'] = data_df.groupby(['date', 'year', 'quarter', 'month', 'day', 'hour']).ngroup() + 1
    data_df['weather_id'] = data_df.groupby(['weather', 'temperature', 'precipitation']).ngroup() + 1
    data_df['event_id'] = data_df['events'].astype('category').cat.codes + 1
    return data_df

transformed_data_df = generate_ids(transformed_data_df)


### Storage back into S3 ###
def save_to_s3(dataframe, bucket, key):
    csv_buffer = io.StringIO()
    dataframe.to_csv(csv_buffer, index=False)
    s3_resource = boto3.resource('s3')
    s3_resource.Object(bucket, key).put(Body=csv_buffer.getvalue())

bucket_name = "cis9440-hw-raw-data"

# Save the transformed data into S3
save_to_s3(transformed_data_df[['date', 'year', 'quarter', 'month', 'day', 'hour', 'time_id']], bucket_name, 'dim_time.csv')
save_to_s3(transformed_data_df[['weather', 'temperature', 'precipitation', 'weather_id']], bucket_name, 'dim_weather.csv')
save_to_s3(transformed_data_df[['events', 'event_id']], bucket_name, 'dim_event.csv')

# Save the fact table to S3
save_to_s3(transformed_data_df[['pedestrians', 'towards_manhattan', 'towards_brooklyn', 'time_id', 'weather_id', 'event_id']], bucket_name, 'fact_pedestrian.csv')

print('Transformed data saved to different CSV files in S3 bucket:', bucket_name)

